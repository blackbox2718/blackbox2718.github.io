---
title: Spectral Clustering
date: 2021-04-30 19:45:00 -0700
math: true
categories: [PIC16B-blog]
tags: [linear-algebra, data-analysis, data-clustering, data-visualization, optimization]
---
In this blog post, we will study and create a [spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering) algorithm which is basically used in exploratory data analysis to divide data points into different groups where each group has their own characteristics/features. 


## $$\S$$1. Introduction


*Spectral clustering* allows us to extract meaningful information from data sets with complex structures. It's one of the most widely used techniques in data exploratory process. Before delving into this, one may ask what kind of data sets that need to be analyzed using spectral clustering. Before addressing that question, let's take a look at some examples where we actually do not need the help from spectral clustering algorithm. Note that all the code in this section is taken from the beginning of Professor Chodrow's [notes](https://github.com/PhilChodrow/PIC16B/blob/master/HW/spectral-clustering.ipynb).


```python
# import some needed libraries for demo
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
# number of data points
n = 200

np.random.seed(1111)
# make two "blobs" of data points
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
# plot 
plt.scatter(X[:,0], X[:,1])
```

    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_2_1.png)
    


Here we observe that the data is divided into two distinct regions ,i.e. two different blobs. In essence, *clustering* refers to the task of separating this data set into the two natural "blobs." ``KMeans`` is a common way to accomplish this task in which it's designed to tackle blobs that have circular shape.


```python
from sklearn.cluster import KMeans

# specify two clusters
km = KMeans(n_clusters = 2)
km.fit(X)

# plot with color indication for each cluster
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```


![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_4_1.png)
    

That looks good. Now, let's explore something a bit more "erratic". Observe the following data set in which when plotted has a crescent shape.


```python
np.random.seed(1234)
n = 200
# generate a data sets with "crescent moon" shape
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
# plot
plt.scatter(X[:,0], X[:,1])
```

    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_6_1.png)
    


We can tell and separate these two data sets pretty easily; however, this is not the case for ``Kmeans`` since its algorithm is not suitable to distinguish clusters with crescent shape. Let's demonstrate this scenario.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```


![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_8_1.png)
    


Without a doubt, we can observe that the division is completely wrong, and k-means is definitely not the best algorithm for this task. So this is where spectral clustering comes to the rescue. In the following sections, we will create and implement the algorithm which would correctly identify the two crescents.

## $$\S$$2. Similarity Matrix

### a. Notation

From now on, we denote

- Boldface capital letters like $$\mathbf{A}$$ refer to matrices (2-dimensional numpy array). 
- Boldface lowercase letters like $$\mathbf{v}$$ refer to vectors (1-dimensional numpy array). 
- $$\mathbf{A}\mathbf{B}$$ refers to a matrix-matrix product (`A@B`).
- $$\mathbf{A}\mathbf{v}$$ refers to a matrix-vector product (`A@v`). 

### b. Construction

One of the first steps we need to perform is to create a *similarity matrix* $$\mathbf{A}$$ in which $$\mathbf{A}_{ij}$$ represents the measure of similarity between data points of indices $$i$$ and $$j$$. There are several ways to measure it, and for this post, we will be using $$\epsilon$$-neighborhood method. Specifically, when constructing the similarity matrix, if the distance between any two entry is within $$\epsilon$$ to each other, then we assign 1 to it with the exception that everything along the diagonal must be 0. For any pairwise distance that is not within the $$\epsilon$$-neighborhood, we assign 0 to those entries.


```python
# a library to compute pairwise distance
from sklearn.metrics import pairwise_distances

epsilon = 0.4

# compute the pairwise distances 
A = pairwise_distances(X)
# any distance entry greater or equal to epsilon is set to 0
A[A >= epsilon] = 0
# any distance entry less than epsilon is set to 1
A[(A != 0) & (A < epsilon)] = 1
# fill the diagonal entries with 0
np.fill_diagonal(A, 0)
# let's take a look
A
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 1., 0.],
           ...,
           [0., 0., 0., ..., 0., 1., 1.],
           [0., 0., 1., ..., 1., 0., 1.],
           [0., 0., 0., ..., 1., 1., 0.]])



## $$\S$$3. Binary Norm Cut Objective

Let's now define several things that we need to compute for the algorithm in the upcoming sections. Notice that we have two clusters of data points, so let $$C_0$$ and $$C_1$$ denote them. Also, we define ``y[i]`` to be either 0 or 1 which indicates which cluster row $$i$$ of $$\mathbf{A}$$ belongs to.

The main objective of this section is to compute the *binary norm cut objective* of a matrix $$\mathbf{A}$$ with two clusters $$C_0$$ and $$C_1$$, which is based on the following mathematical expression

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

where
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$.
- $$\mathbf{vol}(C_n) \equiv \sum_{i \in C_n}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ and $$n = 0, 1$$. The *volume* of cluster $$C_n$$ is a measure of the size of the cluster. 

### a. The Cut Term

The cut term $$\mathbf{cut}(C_0, C_1)$$ represents the numerical relation between cluster $$C_0$$ and $$C_1$$. A small cut indicates that there are significant amount of points in $$C_0$$ which are far away from points in $$C_1$$. Notice that this makes sense mathematically because of the way we define our similarity matrix, i.e. any pairwise distance (entry) which is not within $$\epsilon$$ distance is set to 0.

So, let's define a function to compute the cut term. From the mathematical interpretation of a cut, we can simply do so by summing up all $$A_{ij}$$ for each pair of points $$(i,j)$$ in different clusters, $$C_0$$ and $$C_1$$.


```python
def cut(A, y):
    """
    Input 
    -----
    A(2d numpy array): similarity matrix
    y(1d numpy array): labels
    
    Output
    ------
    numerical value of the cut of the clusters C0 and C1
    """
    cut_sum = 0
    # loop through each row and column entry
    for i in range(n):
        for j in range(n):
            # make sure these are in different clusters
            if y[i] != y[j]:
                cut_sum += A[i, j]
    # counteract the effect of double counting
    return (cut_sum/2)
```

In order to see that the cut objective favors the true label over the random one, we will generate a random 1d numpy array filled with 0 and 1. Then, we will compare the cut term of each one.


```python
rand = np.random.randint(2, size = n)
cut(A, y), cut(A, rand)
```




    (13.0, 1116.0)



Indeed, the cut term of the true labels is considerably smaller than the cut of random label. This can be explained from the fact that the random label array imposes a different data structure on $$A$$ which statistically increases the probability of encountering different clusters within the array. This does lead the cut term of `rand` to be significantly larger than `y`.

### b. The Volume Term 

Recall that the *volume* term gives us a feel of how *big* a cluster is. In order to calculate it, we just need to sum up all rows with the same clustering label (either ``y[i] = 0`` or ``y[i] = 1``). Also, from the formula of norm cut objective, we can observe that the volume has an inverse relationship with it, i.e., the bigger the volume is the smaller the norm cut becomes.

So, let's now write a function which computes that volumes of clusters $$C_0$$ and $$C_1$$.

{::options parse_block_html="true" /}
<div class="got-help">
    
I want to say thank to an anonymous friend who suggests me to shorten this function definition. It definitely looks cleaner than how I defined it initially.
    
```python
def vols(A, y):
    """
    Input
    -----
    A(2d numpy array): similarity matrix
    y(1d numpy array): labels
    
    Output
    ------
    A tuple of volumes of C_0 and C_1
    """
    # gather all rows of cluster 0
    v0 = A[y == 0]
    # gather all rows with cluster 1
    v1 = A[y == 1]
    # compute and return their sum
    return (np.sum(v0), np.sum(v1))
```
</div>
{::options parse_block_html="false" /}

### c. Norm Cut Objective

Recall that the binary normalized cut objective of a similarity matrix $$A$$ with two clusters $$C_0$$ and $$C_1$$ is defined as

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

We can now define a function called ``normcut()`` for the above expression.


```python
def normcut(A, y):
    v0, v1 = vols(A, y)
    return cut(A,y)*(1/v0 + 1/v1)
```

Now, we compute and compare the normcut value of the true label `y` and the artificial label `rand` we generated above.


```python
normcut(A, y), normcut(A, rand)
```




    (0.011518412331615225, 0.995836336735081)



As we may deduce from the value of the cuts between two labels, it's intuitive here that the normcut of the true label is about 100x times smaller than the one which belongs to the random label.

## $$\S$$4. Orthogonal Objective Optimization

Let's define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ where

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

Notice that we now distinguish the two clusters from the sign of $$z$$ in which if $$z_i > 0$$, then it belongs to $$C_0$$, and $$z_i < 0$$ means the $$i$$ component belongs to cluster $$C_1$$.

For any math majors/enthusiasts, you're tasked with showing the following,

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the row sum that we described earlier.

Let's now define a function which is used to compute $$\mathbf{z}$$ using the formula above.


```python
def transform(A, y):
    # create and fill z with some an artificial value
    z = np.ones(n)
    # define z according to the definition
    z[y == 0] = 1/(vols(A, y)[0])
    z[y == 1] = -1/(vols(A, y)[1])
    return z
```

To check whether our functions works and the mathematical expression above is indeed valid, we can check by calculating each side and comparing them. Note that it's numerically impossible for the computer to compute these terms with exact accuracy, so we will use ``np.isclose()`` as an alternative to check (instead of the old-school `` == ``).


```python
# initialize D with an artificial value
D = np.zeros((n, n))
# each entry d_ii in the diagonal corresponds to the sum of each row ith
np.fill_diagonal(D, np.sum(A, axis = 1))

z = transform(A, y)

top = z@(D - A)@z
bottom = z@D@z

# drop the factor of 2 because calculating undirected cut earlier
np.isclose(normcut(A, y), top/bottom)
```




    True



We also need to check whether the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries. 


```python
np.isclose(z.T@D@(np.ones(n)), 0)
```




    True



Since we successfully constructed an alternative expression for the norm cut objective, the problem of minimizing it now becomes the problem of minimizing the following function

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$.

The following code blocks are taken from Professor Chodrow's [prompt](https://github.com/PhilChodrow/PIC16B/blob/master/HW/spectral-clustering.ipynb) for this assignment, where he defines the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$.


```python
# define the orthogonal/normal vectors
def orth(u, v):
    return (u @ v) / (v @ v) * v

# convert D to a 1d array
e = np.ones(n) 
d = D @ e

# define the orthogonal objective which would be used for optimization
def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize
z_ = minimize(orth_obj, z)
```

Now, it's time to visualize what we have achieved so far. Let's see if we're close to cluster the data.


```python
# specify colors for scatter plot based on certain conditions
def clustering_plot(X, sign):
    color = ["purple" if x < -0.0015 else "yellow" for x in sign]
    plt.scatter(X[:, 0], X[:, 1], c = color)
clustering_plot(X, z_.x)
```


    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_35_0.png)
    


Wow, that's pretty close. There's only a small portion of the purple crescent which gets confused to be yellow. So ``minimize()`` does a great job here, but it's still flawed. There's further works that need to be done. Also, notice that we use -0.0015 instead of 0 to differentiate between the negative and positive values due to some numerical errors; we suspect this to be a problem from ``minimize()`` as well since there's a clear distinction between them when we define `z` based on the label of each data point.

## $$\S$$5. Spectral Clustering
### a. The Eivenvectors of Laplacian Matrix

Recall that our ultimate goal is to minimize the following function

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

From [Rayleigh-Ritz Theorem](https://en.wikipedia.org/wiki/Rayleigh%E2%80%93Ritz_method), we know that minimizing $$z$$ is equivalent to finding the second-smallest eigenvalue of the following problem

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

One may certainly ask why it has to be second-smallest eigenvalue instead of the smallest one. In fact, $$\mathbb{1}$$ is the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, so we can simply ignore it and pick the "smallest" value.

The matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$ is often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. 

Let's now find this eigenvalue and call it ``z_eig``.

{::options parse_block_html="true" /}
<div class="got-help">
    
Thank you a PIC16B-mate for pointing out that I should not alter ``eigval`` while attempting to find the second-smallest eigenvalue.
    
```python
# define the Laplacian matrix
L = np.linalg.inv(D)@(D-A)
# compute the eigenvalues and eigenvectors of L
eigval, eigvec = np.linalg.eig(L)
# make a copy of eigval
eigval_c = eigval
# set the smallest eigenvalue to be something greater than itself
eigval_c[np.where(eigval == eigval.min())[0][0]] = 1
# the second smallest eigenvalue now is the min so we can set z_eig accordingly
z_eig = eigvec[:, np.where(eigval_c == eigval_c.min())[0][0]]
```
</div>
{::options parse_block_html="false" /}

We plot the data again to see how ``z_eig`` affects the clustering.


```python
clustering_plot(X, z_eig)
```


    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_40_0.png)
    


Impressive! Spectral clustering works almost perfectly as there is only one point that is purple point mistook to be yellow at the top left.

### b. A Compact Version of Spectral Clustering
Now, let's define a function that sums up everything that we have done from the beginning up to this part. This function can be considered as a compact version of the spectral clustering algorithm.

{::options parse_block_html="true" /}
<div class="gave-help">
    
A pretty concise function definition that I had which catches the eye of some of my reviewers.
    
```python
def spectral_clustering(X, epsilon):
    """
    FUNCTION
    --------
    Perform spectral clustering and generate an array of labels
    which specifies what group a data point in X belongs to
    
    PARAMETERS
    ----------
    X      : input data (numpy array)
    epsilon: distance threshold to build a similarity matrix (float)
    
    RETURN
    ------
    an array of binary labels of each data point in X
    """
    # construct the similarity matrix
    A = pairwise_distances(X)
    A[A >= epsilon] = 0
    A[(A != 0) & (A < epsilon)] = 1
    
    # construct the Laplacian matrix and
    # compute the eigenvector correspond to the second-smallest 
    # eigenvalue of the Laplacian
    D = np.zeros((n, n))
    np.fill_diagonal(D, np.sum(A, axis = 1))
    eigval, eigvec = np.linalg.eig(np.linalg.inv(D)@(D-A))
    
    # eigval is placed in an increasing order so the second column is what we want
    # and the negative values correspond to 1 and positive values correspond to 0
    return (eigvec[:, 1] < 0) * 1
```
</div>
{::options parse_block_html="false" /}

As usual, we need to test to see whether the function works as we expect it to.


```python
plt.scatter(X[:, 0], X[:, 1], c=spectral_clustering(X, epsilon))
```




    <matplotlib.collections.PathCollection at 0x7fa5023e0d30>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_45_1.png)
    


Woo-hoo! We just successfully implemented and packaged a concrete version of spectral clustering algorithm.

### c. Some Experiments
So what's next? At this point, we will run a few more experiments using our function but with different data sets using `make_moons`. In details, we want to investigate more deeply how the ``noise`` affects the accuracy of our algorithm.


```python
# let's increase to 1000 data points as our function is quite fast now
n = 1000
# low noise level
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:, 0], X[:, 1], c=spectral_clustering(X, epsilon=0.4))
```




    <matplotlib.collections.PathCollection at 0x7fa502588a60>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_48_1.png)
    



```python
# + .1 to the noise
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:, 0], X[:, 1], c=spectral_clustering(X, epsilon=0.4))
```




    <matplotlib.collections.PathCollection at 0x7fa50196bd30>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_49_1.png)
    



```python
# + .15 to the noise
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.3, random_state=None)
plt.scatter(X[:, 0], X[:, 1], c=spectral_clustering(X, epsilon=0.4))
```




    <matplotlib.collections.PathCollection at 0x7fa4fbf45190>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_50_1.png)
    



```python
# + .1 to the noise
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.4, random_state=None)
plt.scatter(X[:, 0], X[:, 1], c=spectral_clustering(X, epsilon=0.4))
```




    <matplotlib.collections.PathCollection at 0x7fa4fbe305b0>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_51_1.png)
    

I am glad that a classmate really likes the details I provided in this section

{::options parse_block_html="true" /}
<div class="gave-help">
After this point, if we keep increasing the noise, we will start to encounter the error where the matrix generated is singular which ,mathematically speaking, would prevent us from computing the inverse in one of the steps shown above. An interesting pattern can be observed here is that the higher the noise is the more the shape diverges from a crescent. At ``noise = 0.3``, we can see that the plot no longer looks like a crescent but rather a chunk of data with a not very clear pattern. That's why it makes sense that our algorithm decides to split this shape into half with a straight line.
</div>
{::options parse_block_html="false" /}

To further test the function, why we don't try with a more interesting-shaped data set; in this case, a bull-eye!


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fa4fae9e4c0>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_54_1.png)
    


There are two concentric circles. As we tested before, ``Kmeans`` is not a very good candidate for this task.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fa4fa484790>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_56_1.png)
    


Let's see how our function performs.


```python
plt.scatter(X[:,0], X[:,1], c=spectral_clustering(X, epsilon=0.2))
```




    <matplotlib.collections.PathCollection at 0x7fa4fae34460>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_58_1.png)
    



```python
plt.scatter(X[:,0], X[:,1], c=spectral_clustering(X, epsilon=0.4))
```




    <matplotlib.collections.PathCollection at 0x7fa4facaa5e0>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_59_1.png)
    



```python
plt.scatter(X[:,0], X[:,1], c=spectral_clustering(X, epsilon=0.6))
```




    <matplotlib.collections.PathCollection at 0x7fa4faa0e1f0>




    
![png](/images/2021-04-30-homework2_files/2021-04-30-homework2_60_1.png)
    


It appears that ``epsilon = 0.4`` is the optimal value that supports the function to correctly predict the two regions, which "mysteriously" coincides with the value we chose in the beginning of this blog post to define the similarity matrix. Hmmm...
